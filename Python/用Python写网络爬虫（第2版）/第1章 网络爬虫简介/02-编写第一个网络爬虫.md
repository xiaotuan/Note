[toc]

### 1. 下载网页

下面的示例脚本使用 Python 的 `urllib` 模块下载 URL。

```python
import urllib.request

def download(url):
    return urllib.request.urlopen(url).read()

result = download('http://www.baidu.com')
print(result)
```

> 由于 Python 3.x 以上版本揽括了 urllib2，把 urllib2 和 urllib 整合到了 urllib3 模块中，因此可以通过按住 urllib3 模块即可：
>
> ```shell
> $ pip3 install urllib3
> ```

当下载网页时，可能会遇到一些无法控制的错误。此时，`urllib` 会抛出异常，然后退出脚本。下面在给出一个更稳健的版本，可以捕获这些异常。

```python
import urllib.request
from urllib.error import URLError, HTTPError, ContentTooShortError

def download(url):
    print(f'Downloading: {url}')
    try:
        html = urllib.request.urlopen(url).read()
    except (URLError, HTTPError, ContentTooShortError) as e:
        print('Download error: ', e.reason)
        html = None
    return html

html = download("http://www.baidu.com")
print(html)
```

#### 1.1 重试下载

下面是支持重试下载功能的新版本代码：

```python
'''
    由于 Python 3.x 已经整合 urllib 和 urllib2 到了 urllib3 模块，因此需要使用下面命令安装 urllib3:
    $ pip3 install urllib3
'''
import urllib.request
from urllib.error import URLError, HTTPError, ContentTooShortError

def download(url, num_retries=2):
    print('Downloading:', url)
    request = urllib.request.Request(url)
    request.add_header('User-agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.192 Safari/537.36')
    try:
        html = urllib.request.urlopen(request).read()
    except (URLError, HTTPError, ContentTooShortError) as e:
        print('Download error:', e.reason)
        html = None
        if num_retries > 0:
            if hasattr(e, 'code') and 500 <= e.code < 600:
                # recursively retry 5xx HTTP errors
                return download(url, num_retries - 1)
    return html

html = download("http://httpstat.us/500")
print(html)
```

> 提示：可以使用 <http://httpstat.us/500> 来模拟服务器返回状态码，比如要返回 404 状态码，可以使用 <http://httpstat.us/404>。如果使用 <http://httpstat.us/500> 来模拟，则必须设置代理为正常的浏览器，否则会被服务器拒绝。

#### 1.2 设置用户代理

下面的代码对 `download` 函数进行修改，设定了一个默认的用户代理 `'wswp'`。

```python
'''
    由于 Python 3.x 已经整合 urllib 和 urllib2 到了 urllib3 模块，因此需要使用下面命令安装 urllib3:
    $ pip3 install urllib3
'''
import urllib.request
from urllib.error import URLError, HTTPError, ContentTooShortError

def download(url, user_agent='wswp', num_retries=2):
    print('Downloading: ', url)
    request = urllib.request.Request(url)
    request.add_header('User-agent', user_agent)
    try:
        html = urllib.request.urlopen(request).read()
    except (URLError, HTTPError, ContentTooShortError) as e:
        print('Download error: ', e.reason)
        html = None
        if num_retries > 0:
            if hasattr(e, 'code') and 500 <= e.code < 600:
                # recursively retry 5xx HTTP errors
                return download(url, num_retries - 1)
    return html

html = download("http://www.baidu.com")
print(html)
```

### 2. 网站地图爬虫

下面是该示例爬虫的代码：

```python
import re
import urllib.request
from urllib.error import URLError, HTTPError, ContentTooShortError

def download(url, user_agent='wswp', num_retries=2, charset='utf-8'):
    print('Downloading: ', url)
    request = urllib.request.Request(url)
    request.add_header('User-agent', user_agent)
    try:
        resp = urllib.request.urlopen(request)
        cs = resp.headers.get_content_charset()
        if not cs:
            cs = charset
        html = resp.read().decode(cs)
    except (URLError, HTTPError, ContentTooShortError) as e:
        print('Download error: ', e.reason)
        html = None
        if num_retries > 0:
            if hasattr(e, 'code') and 500 <= e.code < 600:
                # recursively retry 5xx HTTP errors
                return download(url, num_retries - 1)
    return html

def crawl_sitemap(url):
    # download the sitemap file
    sitemap = download(url)
    # extract the sitemap links
    links = re.findall('<loc>(.*?)</loc>', sitemap)
    # download each link
    for link in links:
        html = download(link)
        # scrape html here
        # ...

crawl_sitemap('http://example.python-scraping.com/sitemap.xml')
```

我们的代码依赖于网站维护者在响应头中包含适当的字符编码。如果没有返回字符编码头部，我们将会把它设置默认值 UTF-8。还有一些更复杂的方式用于猜测编码（参见 <https://pypi.python.org/pypi/chardet>）。

### 3. ID 遍历爬虫

下面是一些示例国家（或地区）的 URL。

+ http://example.python-scraping.com/places/default/view/Afghanistan-1
+ http://example.python-scraping.com/places/default/view/Aland-Islands-2
+ http://example.python-scraping.com/places/default/view/Albania-3

可以看出，这些 URL 只在 URL 路径的最后一部分有所区别，包括国家（或地区）名（作为页面别名）和 ID。

下面是使用了该技巧的代码片段。

```python
import itertools
import urllib.request
from urllib.error import URLError, HTTPError, ContentTooShortError

def download(url, user_agent='wswp', num_retries=2, charset='utf-8'):
    print('Downloading: ', url)
    request = urllib.request.Request(url)
    request.add_header('User-agent', user_agent)
    try:
        resp = urllib.request.urlopen(request)
        cs = resp.headers.get_content_charset()
        if not cs:
            cs = charset
        html = resp.read().decode(cs)
    except (URLError, HTTPError, ContentTooShortError) as e:
        print('Download error: ', e.reason)
        html = None
        if num_retries > 0:
            if hasattr(e, 'code') and 500 <= e.code < 600:
                # recursively retry 5xx HTTP errors
                return download(url, num_retries - 1)
    return html

def crawl_site(url):
    for page in itertools.count(1):
        pg_url = '{}{}'.format(url, page)
        html = download(pg_url)
        if html is None:
            break
        # success - can scrape the result

crawl_site('http://example.python-scraping.com/view/-')
```

在这段代码中，我们对 ID 进行遍历，直到出现下载错误时停止，我们假设此时抓取已到达最后一个国家（或地区）的页面。下面是这段代码的改进版本，在该版本中连续发生多次下载错误后才会退出程序。

```python
import itertools
import urllib.request
from urllib.error import URLError, HTTPError, ContentTooShortError

def download(url, user_agent='wswp', num_retries=2, charset='utf-8'):
    print('Downloading: ', url)
    request = urllib.request.Request(url)
    request.add_header('User-agent', user_agent)
    try:
        resp = urllib.request.urlopen(request)
        cs = resp.headers.get_content_charset()
        if not cs:
            cs = charset
        html = resp.read().decode(cs)
    except (URLError, HTTPError, ContentTooShortError) as e:
        print('Download error: ', e.reason)
        html = None
        if num_retries > 0:
            if hasattr(e, 'code') and 500 <= e.code < 600:
                # recursively retry 5xx HTTP errors
                return download(url, num_retries - 1)
    return html

def crawl_site(url, max_errors=5):
    num_errors = 0
    for page in itertools.count(1):
        pg_url = '{}{}'.format(url, page)
        html = download(pg_url)
        if html is None:
            num_errors += 1
            if num_errors == max_errors:
                # max errors reached, exit loop
                break
        else:
            num_errors = 0
        # success - can scrape the result

crawl_site('http://example.python-scraping.com/view/-')
```

### 4. 链接爬虫

通过跟踪每个链接的方式，我们可以很容易地下载整个网站的页面。但是，这种方法可能会下载很多并不需要的网页。我们可以使用正则表达式来确定应当下载哪些页面。下面是这段代码的初始版本。

```python
import re
import itertools
import urllib.request
from urllib.parse import urljoin
from urllib.error import URLError, HTTPError, ContentTooShortError

def download(url, user_agent='wswp', num_retries=2, charset='utf-8'):
    print('Downloading: ', url)
    request = urllib.request.Request(url)
    request.add_header('User-agent', user_agent)
    try:
        resp = urllib.request.urlopen(request)
        cs = resp.headers.get_content_charset()
        if not cs:
            cs = charset
        html = resp.read().decode(cs)
    except (URLError, HTTPError, ContentTooShortError) as e:
        print('Download error: ', e.reason)
        html = None
        if num_retries > 0:
            if hasattr(e, 'code') and 500 <= e.code < 600:
                # recursively retry 5xx HTTP errors
                return download(url, num_retries - 1)
    return html

def link_crawler(start_url, link_regex):
    """
    Crawl from the given start URL following links matched by link_regex
    """
    crawl_queue = [start_url]
    # keep track which URL's have seen before
    seen = set(crawl_queue)
    while crawl_queue:
        url = crawl_queue.pop()
        html = download(url)
        if not html:
            continue
        # filter for links matching our regular expression
        for link in get_links(html):
            # check if link matches expected regex
            if re.match(link_regex, link):
                abs_link = urljoin(start_url, link)
                # check if have already seen this link
                if abs_link not in seen:
                    seen.add(abs_link)
                    print("link: ", abs_link)
                    crawl_queue.append(abs_link)
    print("Done")

def get_links(html):
    """
    Return a list of links from html
    """
    # a regular expression to extract all links from the webpage
    webpage_regex = re.compile("""<a[^>]+href=["'](.*?)["']""", re.IGNORECASE)
    # list of all links from the webpage
    return webpage_regex.findall(html)

link_crawler('http://example.python-scraping.com', '/places/default/(index|view)/')
```

### 5. 高级功能

#### 5.1 解析 robots.txt

使用 Python 的 `urllib` 库中的 `robotparser` 模块，就可以解析 `robots.txt` 文件，如下面的代码所示：

```python
from urllib import robotparser

rp = robotparser.RobotFileParser()
rp.set_url('http://example.python-scraping.com/robots.txt')
rp.read()
url = 'http://example.python-scraping.com'
user_agent = 'BadCrawler'
result = rp.can_fetch(user_agent, url)
print('BadCrawler fetch: ', result)
user_agent = 'GoodCrawler'
result = rp.can_fetch(user_agent, url)
print('GoodCrawler fetch: ', result)
```

下面是链接爬虫的改进，添加根据 `robots.txt` 进行爬取网页：

```python
import re
import itertools
import urllib.request
from urllib import robotparser
from urllib.parse import urljoin
from urllib.error import URLError, HTTPError, ContentTooShortError

def download(url, user_agent='wswp', num_retries=2, charset='utf-8'):
    print('Downloading: ', url)
    request = urllib.request.Request(url)
    request.add_header('User-agent', user_agent)
    try:
        resp = urllib.request.urlopen(request)
        cs = resp.headers.get_content_charset()
        if not cs:
            cs = charset
        html = resp.read().decode(cs)
    except (URLError, HTTPError, ContentTooShortError) as e:
        print('Download error: ', e.reason)
        html = None
        if num_retries > 0:
            if hasattr(e, 'code') and 500 <= e.code < 600:
                # recursively retry 5xx HTTP errors
                return download(url, num_retries - 1)
    return html

def link_crawler(start_url, link_regex, robtos_url=None, user_agent='wswp'):
    """
    Crawl from the given start URL following links matched by link_regex
    """
    crawl_queue = [start_url]
    # keep track which URL's have seen before
    seen = set(crawl_queue)
    if not robtos_url:
        robtos_url = '{}/robots.txt'.format(start_url)
    rp = get_robots_parser(robtos_url)
    while crawl_queue:
        url = crawl_queue.pop()
        # check url passes robots.txt restrictions
        if rp.can_fetch(user_agent, url):
            html = download(url, user_agent=user_agent)
            if not html:
                continue
            # filter for links matching our regular expression
            for link in get_links(html):
                # check if link matches expected regex
                if re.match(link_regex, link):
                    abs_link = urljoin(start_url, link)
                    # check if have already seen this link
                    if abs_link not in seen:
                        seen.add(abs_link)
                        print("link: ", abs_link)
                        crawl_queue.append(abs_link)
        else:
            print('Blocked by robots.txt: ', url)
    print("Done")

def get_links(html):
    """
    Return a list of links from html
    """
    # a regular expression to extract all links from the webpage
    webpage_regex = re.compile("""<a[^>]+href=["'](.*?)["']""", re.IGNORECASE)
    # list of all links from the webpage
    return webpage_regex.findall(html)

def get_robots_parser(robots_url):
    " Return the robots parser object using the robots_url "
    rp = robotparser.RobotFileParser()
    rp.set_url(robots_url)
    rp.read()
    return rp

link_crawler('http://example.python-scraping.com', '/places/default/(index|view)/', user_agent="BadCrawler")
```

#### 5.2 支持代理

Python HTTP 模块—— `requests`，该模块同样也能够处理代理。下面是使用 `urllib` 支持代理的代码：

```python
proxy = 'http://myproxy.net:1234' # example string
proxy_support = urllib.request.ProxyHandler({'http': proxy})
opener = urllib.request.build_opener(proxy_support)
urllib.request.install_opener(opener)
# now request via urllib.request will be handled via proxy
```

下面是集成了该功能的新版本 `download` 函数：

```python
import urllib.request
from urllib.error import URLError, HTTPError, ContentTooShortError

def download(url, user_agent='wswp', num_retries=2, charset='utf-8', proxy=None):
    print('Downloading: ', url)
    request = urllib.request.Request(url)
    request.add_header('User-agent', user_agent)
    try:
        if proxy:
            proxy_support = urllib.request.ProxyHandler({'http': proxy})
            opener = urllib.request.build_opener(proxy_support)
            urllib.request.install_opener(opener)
        resp = urllib.request.urlopen(request)
        cs = resp.headers.get_content_charset()
        if not cs:
            cs = charset
        print("cs: ", cs)
        html = resp.read().decode(cs)
    except (URLError, HTTPError, ContentTooShortError) as e:
        print('Download error: ', e.reason)
        html = None
        if num_retries > 0:
            if hasattr(e, 'code') and 500 <= e.code < 600:
                # recursively retry 5xx HTTP errors
                return download(url, num_retries - 1, charset, proxy)
    return html

html = download("http://www.google.com", proxy="http://127.0.0.1:1081")
print(html)
```

> 注意：目前在默认情况下（Python 3.5），`urllib` 模块不支持 `https` 代理。该问题可能会在 Python 未来的版本中发现变化，隐藏请查阅最新的文档。此外，你还可以使用文档推荐的诀窍（<https://code.activestate.com/recipes/456195>，或继续阅读来学习如何使用 `requests` 库。

#### 5.3 下载限速

下面是实现了该功能的类的代码。

```python
from urllib.parse import urlparse
import time

class Throttle:
    """
    Add a delay between downloads to the same domain
    """
    def __init__(self, delay):
        # amount of delay between downloads for each domain
        self.delay = delay
        # timestamp of when a domain was last accessed
        self.domains = {}

    def wait(self, url):
        domain = urlparse(url).netloc
        last_accessed = self.domains.get(domain)

        if self.delay > 0 and last_accessed is not None:
            sleep_secs = self.delay - (time.time() - last_accessed)
            if sleep_secs > 0:
                # domain has been accessed recently
                # so need to sleep
                time.sleep(sleep_secs)
        # update the last accessed time
        self.domains[domain] = time.time()
```

`Throttle` 类记录了每个域名上次访问的时间，如果当前时间距离上次访问时间小于指定延时，则执行睡眠操作。我们可以在每次下载之前调用 `throttle` 对爬虫进行限速。

```python
throttle = Throttle(delay)
...
throttle.wait(url)
html = download(url, user_agent=user_agent, num_retries=num_retries, proxy=proxy, charset=charset)
```

#### 5.4 避免爬虫陷阱

一些网站会动态生成页面内容，这样就会出现无限多的网页。想要避免陷入爬虫陷阱，一个简单的方法是记录到达当前页面经过了多少个链接，也就是深度。

```python
import re
import itertools
import urllib.request
from urllib import robotparser
from urllib.parse import urljoin
from urllib.error import URLError, HTTPError, ContentTooShortError

def download(url, user_agent='wswp', num_retries=2, charset='utf-8'):
    print('Downloading: ', url)
    request = urllib.request.Request(url)
    request.add_header('User-agent', user_agent)
    try:
        resp = urllib.request.urlopen(request)
        cs = resp.headers.get_content_charset()
        if not cs:
            cs = charset
        html = resp.read().decode(cs)
    except (URLError, HTTPError, ContentTooShortError) as e:
        print('Download error: ', e.reason)
        html = None
        if num_retries > 0:
            if hasattr(e, 'code') and 500 <= e.code < 600:
                # recursively retry 5xx HTTP errors
                return download(url, num_retries - 1)
    return html

def link_crawler(start_url, link_regex, robtos_url=None, user_agent='wswp', max_depth=4):
    """
    Crawl from the given start URL following links matched by link_regex
    """
    crawl_queue = [start_url]
    # keep track which URL's have seen before
    seen = {}
    if not robtos_url:
        robtos_url = '{}/robots.txt'.format(start_url)
    rp = get_robots_parser(robtos_url)
    while crawl_queue:
        url = crawl_queue.pop()
        # check url passes robots.txt restrictions
        if rp.can_fetch(user_agent, url):
            depth = seen.get(url, 0)
            if depth == max_depth:
                print('Skipping %s due to depth' %url)
                continue
            html = download(url, user_agent=user_agent)
            if not html:
                continue
            # filter for links matching our regular expression
            for link in get_links(html):
                # check if link matches expected regex
                if re.match(link_regex, link):
                    abs_link = urljoin(start_url, link)
                    # check if have already seen this link
                    if abs_link not in seen:
                        seen[abs_link] = depth + 1
                        print("link: ", abs_link)
                        crawl_queue.append(abs_link)
        else:
            print('Blocked by robots.txt: ', url)
    print("Done")

def get_links(html):
    """
    Return a list of links from html
    """
    # a regular expression to extract all links from the webpage
    webpage_regex = re.compile("""<a[^>]+href=["'](.*?)["']""", re.IGNORECASE)
    # list of all links from the webpage
    return webpage_regex.findall(html)

def get_robots_parser(robots_url):
    " Return the robots parser object using the robots_url "
    rp = robotparser.RobotFileParser()
    rp.set_url(robots_url)
    rp.read()
    return rp

link_crawler('http://example.python-scraping.com', '/places/default/(index|view)/', max_depth=1)
```

### 6. 使用 requests 库

目前 Python 编写的主流爬虫一般都会使用 `request` 库来管理复杂的 HTTP 请求。

安装 `request` 模块的命令如下：

```shell
pip3 install requests
```

如果想了解其所有功能的进一步介绍，可以阅读它的文档，地址为：<http://python-requests.org>，此外也可以浏览其源代码，地址为：<https://github.com/kennethreitz/requests>。

`requests` 版本如下所示：

```python
import re
import itertools
import requests
import urllib.request
from urllib import robotparser
from urllib.parse import urljoin
from urllib.error import URLError, HTTPError, ContentTooShortError

def download(url, user_agent='wswp', num_retries=2, proxies=None):
    print('Downloading: ', url)
    headers = {'User-Agent': user_agent}
    try:
        resp = requests.get(url, headers=headers, proxies=proxies)
        html = resp.text
        if resp.status_code >= 400:
            print('Download error:', resp.text)
            html = None
            if num_retries and 500 <= resp.status_code < 600:
                # recursively retry 5xx HTTP errors
                return download(url, num_retries - 1)
    except requests.exceptions.RequestException as e:
        print('Download error: ', e.reason)
        html = None
    return html

def link_crawler(start_url, link_regex, robtos_url=None, user_agent='wswp', max_depth=4):
    """
    Crawl from the given start URL following links matched by link_regex
    """
    crawl_queue = [start_url]
    # keep track which URL's have seen before
    seen = {}
    if not robtos_url:
        robtos_url = '{}/robots.txt'.format(start_url)
    rp = get_robots_parser(robtos_url)
    while crawl_queue:
        url = crawl_queue.pop()
        # check url passes robots.txt restrictions
        if rp.can_fetch(user_agent, url):
            depth = seen.get(url, 0)
            if depth == max_depth:
                print('Skipping %s due to depth' %url)
                continue
            html = download(url, user_agent=user_agent)
            if not html:
                continue
            # filter for links matching our regular expression
            for link in get_links(html):
                # check if link matches expected regex
                if re.match(link_regex, link):
                    abs_link = urljoin(start_url, link)
                    # check if have already seen this link
                    if abs_link not in seen:
                        seen[abs_link] = depth + 1
                        print("link: ", abs_link)
                        crawl_queue.append(abs_link)
        else:
            print('Blocked by robots.txt: ', url)
    print("Done")

def get_links(html):
    """
    Return a list of links from html
    """
    # a regular expression to extract all links from the webpage
    webpage_regex = re.compile("""<a[^>]+href=["'](.*?)["']""", re.IGNORECASE)
    # list of all links from the webpage
    return webpage_regex.findall(html)

def get_robots_parser(robots_url):
    " Return the robots parser object using the robots_url "
    rp = robotparser.RobotFileParser()
    rp.set_url(robots_url)
    rp.read()
    return rp

link_crawler('http://example.python-scraping.com', '/places/default/(index|view)/', max_depth=2)
```

`status_code` 的使用更加方便，因为每个请求中都包含该属性。另外，我们不再需要测试字符编码了，因为 `Response` 对象的 `text` 属性已经为我们自动化实现了该功能。代理处理也已经被考虑进来了，我们只需传递代理的字典即可（即 `{'http': 'http://myproxy.net:1234', 'https': 'https://myproxy.net:1234'}`）。

