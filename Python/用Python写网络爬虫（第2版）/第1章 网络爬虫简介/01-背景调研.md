[toc]

网站自身的 `robots.txt` 和 `Sitemap` 文件都可以为我们提供一定的帮助，此外还有一些能提供更详细的外部工具，比如 Google 搜索和 `WHOIS`。

### 1. 检查 robots.txt

大多数网站都会定义 `robots.txt` 文件，这样可以让爬虫了解爬取该网站时存在哪些限制。关于 `robots.txt` 协议的更多信息可以参见 <http://www.robotstxt.org>。下面的代码是我们的示例文件 `robots.txt` 中的内容，可以访问 <http://example.python-scraping.com/robots.txt> 获取。

```txt
# section 1
User-agent: BadCrawler
Disallow: /

# section 2
User-agent: *
Disallow: /trap 
Crawl-delay: 5

# section 3
Sitemap: http://example.python-scraping.com/sitemap.xml
```

在 section 1 中，`robots.txt` 文件禁止用户代理为 BadCrawler 的爬虫爬取该网站。

section 2 规定，无论使用哪种用户代理，都应该在两次下载请求之间给出 5 秒的爬取延迟，我们需要遵从该建议以避免服务器过载。这里还有一个 `/trap` 链接，用于封禁那些爬取了不允许访问的链接的恶意爬虫。如果你访问了这个链接，服务器就会封禁你的 IP 一分钟，或者更长的实际，甚至是永久封禁。

section 3 定义了一个 `Sitemap` 文件。

### 2. 检查网站地图

网站提供的 `Sitemap` 文件可以帮助爬虫定位网站最新的内容，而无须爬取每一个网页。如果想要了解更多信息，可以从 <http://www.sitemaps.org/protocol.html> 获取网站地图标准的定义。下面是在 `robots.txt` 文件中定位到的 `Sitemap` 文件的内容。

```xml
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
    <url>
    	<loc>http://example.python-scraping.com/places/default/view/Afghanistan-1</loc>
    </url>
    <url>
    	<loc>http://example.python-scraping.com/places/default/view/Aland-Islands-2</loc>
    </url>
    <url>
    	<loc>http://example.python-scraping.com/places/default/view/Albania-3</loc>
    </url>
    <url>
    	<loc>http://example.python-scraping.com/places/default/view/Algeria-4</loc>
    </url>
	...
</urlset>
```

虽然 `Sitemap` 文件提供了一种爬取网站的有效方式，但是仍需对其谨慎处理，因为该文件可能存在缺失、过期或不完整的问题。

### 3. 估算网站大小

估算网站大小的一个简单方法是检查 Google 爬虫的结果，因为 Google 很可能已经爬取过我们感兴趣的网站。我们可以通过 Google 搜索的 `site` 关键词过滤域名结果，从而获取该信息。我们可以从 <http://www.google.com/advanced_search> 了解到该接口及其他高级搜索参数的用法。例如在 Google 浏览器搜索框中输入如下内容可以查看百度网站大小信息：

```console
site:www.baidu.com
```

### 4. 识别网站所用技术

`detectem` 模块可以检查网站构建的技术类型，该模块需要 Python 3.5+ 环境以及 Docker。如果你还没有安装 Docker，可以遵照 <https://www.docker.com/products/overview> 中你使用的操作系统所对应的说明操作。当 Docker 安装好后，你可以运行如下命令。

```shell
docker pull scrapinghub/splash
pip3 install detectem
```

上述操作将从 ScrapingHub 拉去最新的 Docker 镜像，并通过 `pip` 安装该库。为了确保不受任何更新或改动的影响，推荐使用 Python 虚拟环境（<https://docs.python.org/3/library/venv.html> ）或 Conda 环境（<https://conda.io/docs/using/envs.html> ），并查看项目的 ReadMe 页面（<https://github.com/spectresearch/detectem>）。

要想运行该模块，只需使用 `det` 命令即可。

```shell
$ det http://example.python-scraping.com
[('jquery', '1.11.0')]
```

Wappalyzer 是一个基于 Node.js 的项目，支持解析不同后端、广告网络、JavaScript 库以及服务器设置。你也可以在 Docker 中运行 Wappalyzer。首先需要下载其 Docker 镜像，运行如下命令。

```shell
$ docker pull wappalyzer/cli
```

然后，你可以从 Docker 实例中运行脚本。

```shell
$ docker run wappalyzer/cli http://example.python-scraping.com
```

输出结果如下：

```json
{'applications':
[{'categories': ['Javascript Frameworks'],
     'confidence': '100',
     'icon': 'Modernizr.png',
     'name': 'Modernizr',
     'version': ''},
 {'categories': ['Web Servers'],
     'confidence': '100',
     'icon': 'Nginx.svg',
     'name': 'Nginx',
     'version': ''},
 {'categories': ['Web Frameworks'],
     'confidence': '100',
     'icon': 'Twitter Bootstrap.png',
     'name': 'Twitter Bootstrap',
     'version': ''},
 {'categories': ['Web Frameworks'],
     'confidence': '100',
     'icon': 'Web2py.png',
     'name': 'Web2py',
     'version': ''},
 {'categories': ['Javascript Frameworks'],
     'confidence': '100',
     'icon': 'jQuery.svg',
     'name': 'jQuery',
     'version': ''},
 {'categories': ['Javascript Frameworks'],
     'confidence': '100',
     'icon': 'jQuery UI.svg',
     'name': 'jQuery UI',
     'version': '1.10.3'},
 {'categories': ['Programming Languages'],
     'confidence': '100',
     'icon': 'Python.png',
     'name': 'Python',
     'version': ''}],
 'originalUrl': 'http://example.python-scraping.com',
 'url': 'http://example.python-scraping.com'}
```

### 5. 寻找网站所有者

我们可以使用 `WHOIS` 协议查询域名的注册者是谁。Python 中有一个针对该协议的封装库，其文档地址为 <https://pypi.python.org/pypi/python-whois>，我们可以通过 `pip` 进行安装。

```shell
pip3 install python-whois
```

下面是使用该模块对 `appspot.com` 这个域名进行 `WHOIS` 查询时返回结果的核心部分。

```shell
>>> import whois
>>> whois.whois('appspot.com')
{
 ...
 "name_servers": [
 "NS1.GOOGLE.COM",
 "NS2.GOOGLE.COM",
 "NS3.GOOGLE.COM",
 "NS4.GOOGLE.COM",
 "ns4.google.com",
 "ns2.google.com",
 "ns1.google.com",
 "ns3.google.com"
 ],
 "org": "Google Inc.",
 "emails": [
 "abusecomplaints@markmonitor.com",
 "dns-admin@google.com"
 ]
 }
```

