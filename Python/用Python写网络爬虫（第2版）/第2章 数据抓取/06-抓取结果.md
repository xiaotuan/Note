[toc]

### 1. 抓取总结

用于测试性能的代码：

```python
FIELDS = ('area', 'population', 'iso', 'country_or_district', 'capital', 'continent',
'tld', 'currency_code', 'currency_name', 'phone', 'postal_code_format',
'postal_code_regex', 'languages', 'neighbours')

import re
def re_scraper(html):
    results = {}
    for field in FIELDS:
        results[field] = re.search('<tr id="places_%s__row">.*?<td class="w2p_fw">(.*?)</td>' % field, html).groups()[0]
    return results

from bs4 import BeautifulSoup
def bs_scraper(html):
    soup = BeautifulSoup(html, 'html5lib')
    results = {}
    for field in FIELDS:
        results[field] = soup.find('table').find('tr', id='places_%s__row' % field).find('td', class_='w2p_fw').text
    return results

from lxml.html import fromstring
def lxml_scraper(html):
    tree = fromstring(html)
    results = {}
    for field in FIELDS:
        results[field] = tree.cssselect('table > tr#places_%s__row > td.w2p_fw' % field)[0].text_content()
    return results

def lxml_xpath_scraper(html):
    tree = fromstring(html)
    results = {}
    for field in FIELDS:
        results[field] = tree.xpath('//tr[@id="places_%s__row"]/td[@class="w2p_fw"]' % field)[0].text_content()
    return results

import re
import time
import requests

def download(url, user_agent='wswp', num_retries=2, proxies=None):
    print('Downloading: ', url)
    headers = {'User-Agent': user_agent}
    try:
        resp = requests.get(url, headers=headers, proxies=proxies)
        html = resp.text
        if resp.status_code >= 400:
            print('Download error:', resp.text)
            html = None
            if num_retries and 500 <= resp.status_code < 600:
                # recursively retry 5xx HTTP errors
                return download(url, num_retries - 1)
    except requests.exceptions.RequestException as e:
        print('Download error: ', e.reason)
        html = None
    return html

NUM_ITERATIONS = 1000 # number of times to test each scraper
url = 'http://example.python-scraping.com/places/default/view/Albania-3'
html = download(url)

scrapers = [
    ('Regular expressions', re_scraper),
    ('BeautifulSoup', bs_scraper),
    ('Lxml', lxml_scraper),
    ('Xpath', lxml_xpath_scraper)
]

for name, scraper in scrapers:
    # record start time of scrape
    start = time.time()
    for i in range(NUM_ITERATIONS):
        if scraper == re_scraper:
            re.purge()
        result = scraper(html)
        # check scraped result is as expected
        assert result['area'] == '28,748 square kilometres'

    # record end time of scrape and output the total
    end = time.time()
    print('%s: %.2f seconds' % (name, end - start))
```

输出结果如下所示：

```console
Downloading:  http://example.python-scraping.com/places/default/view/Albania-3
Regular expressions: 2.36 seconds
BeautifulSoup: 18.83 seconds
Lxml: 3.51 seconds
Xpath: 1.89 seconds
```

### 2. 为链接爬虫添加抓取回调

```python
import re
import itertools
from lxml.html import fromstring
import urllib.request
from urllib import robotparser
from urllib.parse import urljoin
from urllib.error import URLError, HTTPError, ContentTooShortError

def download(url, user_agent='wswp', num_retries=2, charset='utf-8'):
    print('Downloading: ', url)
    request = urllib.request.Request(url)
    request.add_header('User-agent', user_agent)
    try:
        resp = urllib.request.urlopen(request)
        cs = resp.headers.get_content_charset()
        if not cs:
            cs = charset
        html = resp.read().decode(cs)
    except (URLError, HTTPError, ContentTooShortError) as e:
        print('Download error: ', e.reason)
        html = None
        if num_retries > 0:
            if hasattr(e, 'code') and 500 <= e.code < 600:
                # recursively retry 5xx HTTP errors
                return download(url, num_retries - 1)
    return html

def link_crawler(start_url, link_regex, robtos_url=None, user_agent='wswp', max_depth=4, scrape_callback=None):
    """
    Crawl from the given start URL following links matched by link_regex
    """
    crawl_queue = [start_url]
    # keep track which URL's have seen before
    seen = {}
    data = []
    if not robtos_url:
        robtos_url = '{}/robots.txt'.format(start_url)
    rp = get_robots_parser(robtos_url)
    while crawl_queue:
        url = crawl_queue.pop()
        # check url passes robots.txt restrictions
        if rp.can_fetch(user_agent, url):
            depth = seen.get(url, 0)
            if depth == max_depth:
                print('Skipping %s due to depth' %url)
                continue
            html = download(url, user_agent=user_agent)
            if not html:
                continue
            if scrape_callback:
                data.extend(scrape_callback(url, html) or [])
            # filter for links matching our regular expression
            for link in get_links(html):
                # check if link matches expected regex
                if re.match(link_regex, link):
                    abs_link = urljoin(start_url, link)
                    # check if have already seen this link
                    if abs_link not in seen:
                        seen[abs_link] = depth + 1
                        print("link: ", abs_link)
                        crawl_queue.append(abs_link)
        else:
            print('Blocked by robots.txt: ', url)

def get_links(html):
    """
    Return a list of links from html
    """
    # a regular expression to extract all links from the webpage
    webpage_regex = re.compile("""<a[^>]+href=["'](.*?)["']""", re.IGNORECASE)
    # list of all links from the webpage
    return webpage_regex.findall(html)

def get_robots_parser(robots_url):
    " Return the robots parser object using the robots_url "
    rp = robotparser.RobotFileParser()
    rp.set_url(robots_url)
    rp.read()
    return rp

def scrape_callback(url, html):
    fields = ('area', 'population', 'iso', 'country_or_district', 'capital',
    'continent', 'tld', 'currency_code', 'currency_name',
    'phone', 'postal_code_format', 'postal_code_regex',
    'languages', 'neighbours')
    if re.search('/view/', url):
        tree = fromstring(html)
        all_rows = [
            tree.xpath('//tr[@id="places_%s__row"]/td[@class="w2p_fw"]' % field)[0].text_content()
            for field in fields
        ]
        print(url, all_rows)

link_crawler('http://example.python-scraping.com', '/places/default/(index|view)/', max_depth=2, scrape_callback=scrape_callback)
```

可以将回调做成类，代码如下所示：

```python
import csv
import re
from lxml.html import fromstring

class CsvCallback:
    
    def __init__(self):
        self.writer = csv.writer(open('../data/countries_or_districts.csv', 'w'))
        self.fields = ('area', 'population', 'iso', 'country_or_district', 'capital',
                       'continent', 'tld', 'currency_code', 'currency_name',
                       'phone', 'postal_code_format', 'postal_code_regex',
                       'languages', 'neighbours')
        self.writer.writerow(self.fields)

    def __call__(self, url, html):
        if re.search('/view/', url):
            tree = fromstring(html)
            all_rows = [
                tree.xpath('//tr[@id="places_%s__row"]/td[@class="w2p_fw"]' % field)[0].text_content()
                for field in self.fields]
            self.writer.writerow(all_rows)
```

