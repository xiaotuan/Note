[toc]

### 1. 正则表达式

如果你对正则表达式还不熟悉，或是需要一些提示，那么你可以查阅 <https://docs.python.org/2/howto/regex.html> 获得完整介绍。

当我们使用正则表达式抓取国家（或地区）面积数据时，首先需要尝试匹配 `<td>` 元素中的内容，如下所示：

```python
import re
import requests

def download(url, user_agent='wswp', num_retries=2, proxies=None):
    print('Downloading: ', url)
    headers = {'User-Agent': user_agent}
    try:
        resp = requests.get(url, headers=headers, proxies=proxies)
        html = resp.text
        if resp.status_code >= 400:
            print('Download error:', resp.text)
            html = None
            if num_retries and 500 <= resp.status_code < 600:
                # recursively retry 5xx HTTP errors
                return download(url, num_retries - 1)
    except requests.exceptions.RequestException as e:
        print('Download error: ', e.reason)
        html = None
    return html

url = 'http://example.python-scraping.com/view/UnitedKingdom-239'
html = download(url)
result = re.findall(r'<td class="w2p_fw">(.*?)</td>', html)[1]
print(result)
```

输出结果如下：

```json
['<img src="/places/static/images/flags/va.png" />', 
 '0 square kilometres', 
 '921', 
 'VA', 
 'Vatican', 
 'Vatican City', 
 '<a href="/places/default/continent/EU">EU</a>', 
 '.va', 
 'EUR', 
 'Euro', 
 '379', 
 '#####', 
 '^(\\d{5})$', 
 'la,it,fr', 
 '<div><a href="/places/default/iso/IT">IT </a></div>']
```

想要该正则表达式更加明确，我们可以将其父元素 `<tr>` 也加入进来，由于该元素具有 ID 属性，所以应该是唯一的。

```python
>>> re.findall('<tr id="places_area__row"><td class="w2p_fl"><label
for="places_area" id="places_area__label">Area: </label></td><td
class="w2p_fw">(.*?)</td>', html)
['244,820 square kilometres']
```

下面是尝试支持这些可能性的改进版本。

```python
>>> re.findall('''<tr
id="places_area__row">.*?<tds*class=["']w2p_fw["']>(.*?)</td>''', html)
['244,820 square kilometres']
```

### 2. Beautiful Soup

**Beautiful Soup** 是一个非常流行的 Python 库，它可以解析网页，并提供了定位内容的便捷接口。安装 Beautiful Soup 模块的命令如下：

```python
pip3 install beautifulsoup4
```

由于许多网页都不具备良好的 HTML 格式，因此 Beautiful Soup 需要对其标签开合状态进行修正。

```python
from bs4 import BeautifulSoup
from pprint import pprint

broken_html = '<ul class=country_or_district><li>Area<li>Population</ul>'
# parse the HTML
soup = BeautifulSoup(broken_html, 'html.parser')
fixed_html = soup.prettify()
pprint(fixed_html)
```

输出结果如下：

```console
('<ul class="country_or_district">\n'
 ' <li>\n'
 '  Area\n'
 '  <li>\n'
 '   Population\n'
 '  </li>\n'
 ' </li>\n'
 '</ul>')
```

我们可以看到，使用默认的 `html.parser` 并没有得到正确解析的 HTML。我们还有其他解析器可以选择。我们可以安装 LXML，或者使用 `html5lib`。安装 `html5lib` 命令如下：

```shell
pip3 install html5lib
```

使用 `html5lib` 解析器的代码如下所示：

```python
from bs4 import BeautifulSoup
from pprint import pprint

broken_html = '<ul class=country_or_district><li>Area<li>Population</ul>'
# parse the HTML
soup = BeautifulSoup(broken_html, 'html5lib')
fixed_html = soup.prettify()
pprint(fixed_html)
```

输出结果如下所示：

```json
('<html>\n'
 ' <head>\n'
 ' </head>\n'
 ' <body>\n'
 '  <ul class="country_or_district">\n'
 '   <li>\n'
 '    Area\n'
 '   </li>\n'
 '   <li>\n'
 '    Population\n'
 '   </li>\n'
 '  </ul>\n'
 ' </body>\n'
 '</html>')
```

当你使用 `lxml` 时，也可以看到类似的结果。

现在，我们可以使用 `find()` 和 `find_all()` 方法来定位我们需要的元素了。

```python
from bs4 import BeautifulSoup
from pprint import pprint

broken_html = '<ul class=country_or_district><li>Area<li>Population</ul>'
# parse the HTML
soup = BeautifulSoup(broken_html, 'html5lib')
fixed_html = soup.prettify()
ul = soup.find('ul', attrs={'class': 'country_or_district'})
result = ul.find('li') # returns just the first match
pprint(result)
result = ul.find_all('li')  # returns all matches
pprint(result)
```

输出结果如下所示：

```console
<li>Area</li>
[<li>Area</li>, <li>Population</li>]
```

下面是使用该方法抽取示例网站中国家（或地区）面积数据的完整代码。

```python
import re
import requests
from bs4 import BeautifulSoup

def download(url, user_agent='wswp', num_retries=2, proxies=None):
    print('Downloading: ', url)
    headers = {'User-Agent': user_agent}
    try:
        resp = requests.get(url, headers=headers, proxies=proxies)
        html = resp.text
        if resp.status_code >= 400:
            print('Download error:', resp.text)
            html = None
            if num_retries and 500 <= resp.status_code < 600:
                # recursively retry 5xx HTTP errors
                return download(url, num_retries - 1)
    except requests.exceptions.RequestException as e:
        print('Download error: ', e.reason)
        html = None
    return html

url = 'http://example.python-scraping.com/places/default/view/Albania-3'
html = download(url)
soup = BeautifulSoup(html, features="html5lib")
# locate the area row
tr = soup.find(attrs={'id':'places_area__row'})
td = tr.find(attrs={'class':'w2p_fw'})  # locate the data element
area = td.text  # extract the text from the data element
print(area)
```

### 3. Lxml

**Lxml** 是基于 `libxml2` 这一 XML 解析库构建的 Python 库，它使用 C 语言编写，解析速度比 Beautiful Soup 更快。`lxml` 模块的安装命令如下：

```shell
pip3 install lxml
```

下面是使用该模块解析同一个不完整 HTML 的例子。

```python
from lxml.html import fromstring, tostring

broken_html = '<ul class=country_or_district><li>Area<li>Population</ul>'
tree = fromstring(broken_html)  # parse the HTML
fixed_html = tostring(tree, pretty_print=True)
print(fixed_html)
```

输出结果如下所示：

```console
b'<ul class="country_or_district">\n<li>Area</li>\n<li>Population</li>\n</ul>\n'
```

要想使用 CSS 选择器，你可能需要先安装 `cssselect` 库，如下所示：

```shell
pip3 install cssselect
```

我们可以使用 `lxml` 的 CSS 选择器，抽取示例页面中的面积数据了。

```python
import re
import requests
from lxml.html import fromstring, tostring

def download(url, user_agent='wswp', num_retries=2, proxies=None):
    print('Downloading: ', url)
    headers = {'User-Agent': user_agent}
    try:
        resp = requests.get(url, headers=headers, proxies=proxies)
        html = resp.text
        if resp.status_code >= 400:
            print('Download error:', resp.text)
            html = None
            if num_retries and 500 <= resp.status_code < 600:
                # recursively retry 5xx HTTP errors
                return download(url, num_retries - 1)
    except requests.exceptions.RequestException as e:
        print('Download error: ', e.reason)
        html = None
    return html

url = 'http://example.python-scraping.com/places/default/view/Albania-3'
html = download(url)
tree = fromstring(html)
td = tree.cssselect('tr#places_area__row > td.w2p_fw')[0]
area = td.text_content()
print(area)
```

