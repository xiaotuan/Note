[toc]

### 1. 正则表达式

首先需要尝试匹配 `<td>` 元素中的内容，如下所示：

```python
import urllib3
import re

def download(url, user_agent='wswp', num_retries=2):
    print('Downloading:' + url)
    headers = {'User-agent': user_agent}
    http = urllib3.PoolManager()
    request = http.request('GET', url, headers=headers)
    try:
        html = request.data.decode()
    except urllib3.URLError as e:
        print('Download error:', e.reason)
        html = None
        if num_retries > 0:
            if hasattr(e, 'code') and 500 <= e.code < 600:
                # retry 5xx HTTP errorspi
                return download(url, user_agent, num_retries - 1)
    return html


url = 'http://127.0.0.1/places/default/view/United-Kingdom-239'
html = download(url)
print(html)
data = re.findall('<td class="w2p_fw">(.*?)</td>', html)[1]
print(data)
```

输出结果为：

```console
244820 square kilometres
```

想要该正则表达式更加健壮，我们可以将其父元素 `<tr>` 也记入进来：

```python
data = re.findall('<tr id="places_area__row"><td class="w2p_fl"><label class="readonly" for="places_area" id="places_area__label">Area: </label></td><td class="w2p_fw">(.*?)</td>', html)

# ['244820 square kilometres']
```

`<td>` 标签之间添加多余的空格，或是变更 area_label 等。下面是尝试支持这些可能性的改进版本。

```python
data = re.findall('<tr id="places_area__row">.*?<td\s*class=["\']w2p_fw["\']>(.*?)</td>', html)
```

### 2. Beautiful Soup

Beautiful Soup 是一个非常流行的 Python 模块。安装命令如下：

```console
$ pip install beautifulsoup4
```

使用 Beautiful Soup 格式化 HTML 代码如下：

```python
from bs4 import BeautifulSoup

broken_html = '<ul class=country><li>Area<li>Population</ul>'
# parse the HTML
soup = BeautifulSoup(broken_html, 'html.parser')
fixed_html = soup.prettify()
print(fixed_html)

"""
<ul class="country">
 <li>
  Area
  <li>
   Population
  </li>
 </li>
</ul>
"""
```

现在可以使用 `find()` 和 `find_all()` 方法来定位我们需要的元素了。

```python
ul = soup.find('ul', attrs={'class':'country'})
lis = ul.find_all('li')    # retruns just the first match
print(lis)

"""
[<li>Area<li>Population</li></li>, <li>Population</li>]
"""
```

> 提示
>
> 想要了解全部方法和参数，可以查阅 BeautifulSoup 的官方文档，其网址为：<http://www.crummy.com/software/BeautifulSoup/bs4/doc/>。

下面 使用该方法抽取示例国家面积数据的完整代码。

```python
import urllib3
from bs4 import BeautifulSoup

def download(url, user_agent='wswp', num_retries=2):
    print('Downloading:' + url)
    headers = {'User-agent': user_agent}
    http = urllib3.PoolManager()
    request = http.request('GET', url, headers=headers)
    try:
        html = request.data.decode()
    except urllib3.URLError as e:
        print('Download error:', e.reason)
        html = None
        if num_retries > 0:
            if hasattr(e, 'code') and 500 <= e.code < 600:
                # retry 5xx HTTP errorspi
                return download(url, user_agent, num_retries - 1)
    return html

url = 'http://127.0.0.1/places/default/view/United-Kingdom-239'
html = download(url)
soup = BeautifulSoup(html, features="html.parser")
# locate the area row
tr = soup.find(attrs={'id':'places_area__row'})
td = tr.find(attrs={'class':'w2p_fw'})  # locate the area tag
area = td.text  # extract the text from this tag
print(area)

"""
244820 square kilometres
"""
```

### 3. Lxml

最新的安装说明可以参考 <https://lxml.de/installation.html>。安装方法如下：

```console
$ pip install lxml
$ pip install cssselect
```

下面 使用该模块解析同一个不完整 HTML 的例子:

```python
import lxml.html

broken_html = '<ul class=country><li>Area<li>Population</lu>'
tree = lxml.html.fromstring(broken_html)    # parse the HTML
fixed_html = lxml.html.tostring(tree, pretty_print=True)
print(fixed_html)

"""
b'<ul class="country">\n<li>Area</li>\n<li>Population</li>\n</ul>\n'
"""
```

下面是使用 lxml 的 CSS 选择器抽取面积数据的示例代码：

```python
import urllib3
import lxml.html
from lxml.cssselect import CSSSelector

def download(url, user_agent='wswp', num_retries=2):
    print('Downloading:' + url)
    headers = {'User-agent': user_agent}
    http = urllib3.PoolManager()
    request = http.request('GET', url, headers=headers)
    try:
        html = request.data.decode()
    except urllib3.URLError as e:
        print('Download error:', e.reason)
        html = None
        if num_retries > 0:
            if hasattr(e, 'code') and 500 <= e.code < 600:
                # retry 5xx HTTP errorspi
                return download(url, user_agent, num_retries - 1)
    return html

url = 'http://127.0.0.1/places/default/view/United-Kingdom-239'
html = download(url)
tree = lxml.html.fromstring(html)
td = tree.cssselect('tr#places_area__row > td.w2p_fw')[0]
area = td.text_content()
print(area)

"""
244820 square kilometres
"""
```

### 4. CSS选择器

下面是一些常用的选择器示例：

```console
选择所有标签：*
选择<a>标签：a
选择所有class="link"的元素：.link
选择class="link"的<a>标签： a.link
选择id="home"的<a>标签：a#home
选择父元素为<a>标签的所有<span>子标签：a > span
选择<a>标签内部的所有<span>标签：a span
选择title属性为"Home"的所有<a>标签：a[title=Home]
```

> 提示：
>
> W3C 已提出 CSS3 规范，其网址为 <http://www.w3.org/TR/2011/REC-css3-selectors-20110929/>。

Lxml 已经实现了大部分 CSS3 属性，其不支持的功能可以参见 <https://cssselect.readthedocs.io/en/latest/>。

### 5. 性能对比

下面 是使用上述信息抽取所有可用国家数据的实现代码：

```python
FIELDS = ('area', 'population', 'iso', 'country', 'capital', 'continent', 'tld', 'currency_code',
          'currency_name', 'phone', 'postal_code_format', 'postal_code_regex',
          'languages', 'neighbours')

import urllib3

def download(url, user_agent='wswp', num_retries=2):
    print('Downloading:' + url)
    headers = {'User-agent': user_agent}
    http = urllib3.PoolManager()
    request = http.request('GET', url, headers=headers)
    try:
        html = request.data.decode()
    except urllib3.URLError as e:
        print('Download error:', e.reason)
        html = None
        if num_retries > 0:
            if hasattr(e, 'code') and 500 <= e.code < 600:
                # retry 5xx HTTP errorspi
                return download(url, user_agent, num_retries - 1)
    return html

import re

def re_scraper(html):
    results = {}
    for field in FIELDS:
        results[field] = re.search('<tr id="places_%s__row">.*?<td class="w2p_fw">(.*?)</td>' % field, html).groups()[0]
    return results

from bs4 import BeautifulSoup

def bs_scraper(html):
    soup = BeautifulSoup(html, 'html.parser')
    results = {}
    for field in FIELDS:
        results[field] = soup.find('table').find('tr', id='places_%s__row' % field).find('td', class_='w2p_fw').text
    return results

import lxml.html
from lxml.cssselect import CSSSelector

def lxml_scraper(html):
    tree = lxml.html.fromstring(html)
    results = {}
    for field in FIELDS:
        results[field] = tree.cssselect('table > tr#places_%s__row > td.w2p_fw' % field)[0].text_content()
    return results

import time

NUM_ITERATIONS = 100    # number of times to test each scraper
html = download('http://127.0.0.1/places/default/view/United-Kingdom-239')
for name, scraper in [('Regular expressions', re_scraper),
                      ('BeautifulSoup', bs_scraper),
                      ('Lxml', lxml_scraper)]:
    # record start time of scrape
    start = time.time()
    for i in range(NUM_ITERATIONS):
        if scraper == re_scraper:
            re.purge()
        result = scraper(html)
        # check scraped result is as expected
        assert(result['area'] == '244820 square kilometres')
    # record end time of scrape and output the total
    end = time.time()
    print('%s: %.2f seconds' % (name, end - start))
```

下面是运行结果：

```console
Regular expressions: 0.25 seconds
BeautifulSoup: 1.12 seconds
Lxml: 0.38 seconds
```

### 6. 为链接爬虫添加抓取回调

我们需要添加一个 callback 参数处理抓取行为。下面是其实现代码，可以看出在 Python 中实现该功能非常简单：

```python
def link_crawler(..., scrape_callback=None):
    ...
    links = []
    if scrape_callback:
        links.extend(scrape_callback(url, html) or [])
        ...
```

