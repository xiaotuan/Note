下面是演示解析网站地图的示例代码：

```python
import urllib2
import re

def download(url, user_agent='wswp', num_retries=2):
	print 'Downloading:', url
	headers = {'User-agent': user_agent }
	request = urllib2.Request(url, headers=headers)
	try:
		html = urllib2.urlopen(request).read()
	except urllib2.URLError as e:
		print 'Download error:', e.reason
		html = None
		if num_retries > 0:
			if hasattr(e, 'code') and 500 <= e.code < 600:
				# retry 5xx HTTP errors
				return download(url, user_agent, num_retries - 1)
	return html
	
def crawl_sitemap(url):
	# download the sitemap file
	sitemap = download(url)
	#extract the sitemap links
	links = re.findall('<loc>(.*?)</loc>', sitemap)
	# download each link
	for link in links:
		print link
		html = download(link)
		# scrape html here

crawl_sitemap('http://example.webscraping.com/sitemap.xml')

```

下面例子只遍历 ID 来下载所有国家的页面：

```python
import urllib2
import itertools

def download(url, user_agent='wswp', num_retries=2):
	print 'Downloading:', url
	headers = {'User-agent': user_agent }
	request = urllib2.Request(url, headers=headers)
	try:
		html = urllib2.urlopen(request).read()
	except urllib2.URLError as e:
		print 'Download error:', e.reason
		html = None
		if num_retries > 0:
			if hasattr(e, 'code') and 500 <= e.code < 600:
				# retry 5xx HTTP errors
				return download(url, user_agent, num_retries - 1)
	return html
	
for page in itertools.count(1):
	url = 'http://example.webscraping.com/view/-%d' % page
	html = download(url)
	if html is None:
		break
	else:
		# success - can scrape the result
		pass
```

上面的代码假设所有页面都是连续的，当出现下载错误后，将停止运行。

下面是这段代码的改进版本，在该版本中连续发生多次下载错误后才会退出程序：

```python
import urllib2
import itertools

def download(url, user_agent='wswp', num_retries=2):
	print 'Downloading:', url
	headers = {'User-agent': user_agent }
	request = urllib2.Request(url, headers=headers)
	try:
		html = urllib2.urlopen(request).read()
	except urllib2.URLError as e:
		print 'Download error:', e.reason
		html = None
		if num_retries > 0:
			if hasattr(e, 'code') and 500 <= e.code < 600:
				# retry 5xx HTTP errors
				return download(url, user_agent, num_retries - 1)
	return html
	
# maximum number of consecutive download errors allowed 
max_errors = 5
# current number of consecutive download errors
num_errors = 0
for page in itertools.count(1):
	url = 'http://example.webscraping.com/view/-%d' % page
	html = download(url)
	if html is None:
		# received an error trying to download this webpage
		num_errors += 1
		if num_errors == max_errors:
			# reached maximum number of
			# consecutive errors so exit
			break
	else:
		# success - can scrape the result
		#...
		num_errors = 0
```

