可以使用 Python 的 urllib2 模块下载 URL：

```python
import urllib2

def download(url):
    return urllib2.urlopen(url).read()
```

> 注意：urllib2 模块不能在代理下获得正确结果。

urllib2 有可能会抛出异常，安全起见，下面再给出一个更健壮的版本，可以捕获这些异常。

```python
import urllib2

def download(url):
    print 'Downloading:', url
    try:
        html = urllib2.urlopen(url).read()
    except urllib2.URLError as e:
        print 'Download error:', e.reason
        html = None
    return html
```

> 注意：如果 url 是 https 协议的话，可能获取不到有用的信息，这时可以改用 http 协议来下载。

下面是支持重试下载功能的新版本：

```python
import urllib2

def download(url, num_retries = 2):
	print 'Downloading:', url
	try:
		html = urllib2.urlopen(url).read()
	except urllib2.URLError as e:
		print 'Download error:', e.reason
		html = None
		if num_retries > 0:
			if hasattr(e, 'code') and 500 <= e.code < 600:
				# recursively retry 5xx HTTP errors
				return download(url, num_retries - 1)
	return html
	
print download('http://www.baidu.com')
```

> 注意：在输入 python 代码时，一定要注意缩进的一致性。如果确认代码没有问题，但是实际运行出错的话，就应该检查下代码缩进是否不一致。

下面的代码对 download 函数进行了修改，设定了一个默认的用户代理 "wswp"，（即 Web Scraping with Python 的首字母缩写）。

```python
import urllib2

def download(url, user_agent='wswp', num_retries=2):
	print 'Downloading:', url
	headers = {'User-agent': user_agent }
	request = urllib2.Request(url, headers=headers)
	try:
		html = urllib2.urlopen(request).read()
	except urllib2.URLError as e:
		print 'Download error:', e.reason
		html = None
		if num_retries > 0:
			if hasattr(e, 'code') and 500 <= e.code < 600:
				# retry 5xx HTTP errors
				return download(url, user_agent, num_retries - 1)
	return html
	
print download('http://www.baidu.com', 'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.122 Safari/537.36 SE 2.X MetaSr 1.0')
```

