### 第11章　使用Scrapyd与实时分析进行分布式爬取

我们已经走了很长的一段路。我们首先熟悉了两种基础的网络技术——HTML和XPath，然后开始使用Scrapy爬取复杂网站。接下来，我们深入了解了Scrapy通过其设置为我们提供的诸多功能，然后在探讨其Twisted引擎的内部架构和异步功能时，更加深入地了解了Scrapy和Python。在上一章中，我们研究了Scrapy的性能，并学习了如何解决复杂和经常违背直觉的性能问题。

在最后的这一章中，我将为你指出如何进一步将该技术扩展到多台服务器的一些方向。我们很快就会发现爬取工作经常是一种“高度并发”的问题，因此可以轻松地实现横向扩展，利用多台服务器的资源。为了实现该目标，我们可以像平时那样使用一个Scrapy中间件，不过也可以使用Scrapyd，这是一个专门用于管理运行在远程服务器上的Scrapy爬虫的应用。这将允许我们在自己的服务器上，拥有与第6章中介绍的相兼容的功能。

最后，我们将使用基于Apache Spark的简单系统，对抽取的数据执行实时分析。Apache Spark是一个非常流行的大数据处理框架。我们将使用Spark Streaming API展示在数据收集增多时越来越准确的结果。对于我来说，最终的这个应用展示了Python作为一种语言的能力和成熟度，因为我们只需这些，就能编写出富有表现力、简洁并且高效的代码，实现从数据抽取到分析的全栈工作。

