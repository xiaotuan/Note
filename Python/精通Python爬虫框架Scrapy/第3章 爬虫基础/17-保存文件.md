### 3.3.4　保存文件

请尝试如下爬取示例。

```python
$ scrapy crawl basic -o items.json
$ cat items.json
[{"price": ["334.39"], "address": ["Angel, London"], "description": 
["website court ... offered"], "image_urls": [".../images/i01.jpg"], 
"title": ["set unique family well"]}]
$ scrapy crawl basic -o items.jl
$ cat items.jl
{"price": ["334.39"], "address": ["Angel, London"], "description": 
["website court ... offered"], "image_urls": [".../images/i01.jpg"], 
"title": ["set unique family well"]}
$ scrapy crawl basic -o items.csv
$ cat items.csv
description,title,url,price,spider,image_urls...
"...offered",set unique family well,,334.39,,.../images/i01.jpg
$ scrapy crawl basic -o items.xml
$ cat items.xml
<?xml version="1.0" encoding="utf-8"?>
<items><item><price><value>334.39</value></price>...</item></items>

```

我们不需要编写任何额外的代码，就可以保存为这些不同的格式。Scrapy在幕后识别你想要输出的文件扩展名，并以适当的格式输出到文件中。前面的格式覆盖了一些最常见的用例。CSV和XML文件非常流行，因为类似微软Excel的电子表格程序可以直接打开它们。JSON文件在网上非常流行，原因是它们富有表现力而且与JavaScript的关系相当密切。JSON与JSON行（JSON Line）格式的轻微不同是， `.json` 文件是在一个大数组中存储JSON对象的。这就意味着如果你有一个1GB的文件，你可能不得不在使用典型的解析器解析之前，将其全部存入内存当中。而 `.jl` 文件则是每行包含一个JSON对象，所以它们可以被更高效地读取。

将你生成的文件保存到文件系统之外的地方也很容易。比如，通过使用如下命令，Scrapy可以自动将文件上传到FTP或S3存储桶中。

```python
$ scrapy crawl basic -o "ftp://user:pass@ftp.scrapybook.com/items.json "
$ scrapy crawl basic -o "s3://aws_key:aws_secret@scrapybook/items.json"

```

需要注意的是，除非凭证和URL都更新为与有效的主机/S3提供商相匹配，否则该示例无法工作。

> <img class="my_markdown" src="../images/2.png" style="width:69px;  height: 87px; " width="8%"/>
> 我的MySQL驱动在哪里？起初，我也对Scrapy缺少针对MySQL或其他数据库的内置支持感到惊讶。而实际上，没有什么是内置的，这与Scrapy的思考方式是完全违背的。Scrapy的目标是快速和可扩展。它使用了很少的CPU，以及尽可能高的入站带宽。从性能的角度来看，将数据插入到大部分关系型数据库将会是一场灾难。当需要将item插入到数据库时，必须将其先存储到文件当中，然后再使用批量加载机制导入它们。在第9章中，我们将会看到多种高效的方式，用来将独立的item导入到数据库中。

这里需要注意的另一件事是，如果你现在尝试使用 `scrapy parse` ，它会向你显示已经抓取的item，以及你的爬取生成的新请求（本例中没有）。

```python
$ scrapy parse --spider=basic http://web:9312/properties/property_000001.
html
INFO: Scrapy 1.0.3 started (bot: properties)
...
INFO: Spider closed (finished)
>>> STATUS DEPTH LEVEL 1 <<<
# Scraped Items ------------------------------------------------
[{'address': [u'Plaistow, London'],
　'description': [u'features'],
　'image_urls': [u'.../images/i02.jpg'],
　'price': [u'388.03'],
　'title': [u'belsize marylebone...deal']}]
# Requests ------------------------------------------------
[]

```

在调试给出意料之外的结果的URL时，你会更加感激 `scrapy parse` 。

