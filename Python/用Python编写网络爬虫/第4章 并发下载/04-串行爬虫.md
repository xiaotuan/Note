[toc]

### 4.2　串行爬虫

现在我们可以对之前开发的链接爬虫进行少量修改，使用 `AlexaCallback` 串行下载Alexa的前500个URL。

为了更新链接爬虫，现在需要传入起始URL或起始URL列表。

```python
# In link_crawler function
if isinstance(start_url, list):
    crawl_queue = start_url
else:
    crawl_queue = [start_url]
```

我们还需要更新对每个站点中 `robots.txt` 的处理方式。我们使用一个简单的字典来存储每个域名的解析器（参见本书源码文件中chp4文件夹中的 `advanced_link_crawler.py#L53-L72` ）。我们还需处理如下情况：我们遇到的URL不一定是相对路径，甚至部分是无法访问的URL，比如包含 `mailto:` 的邮箱地址或包含 `javascript:` 的事件命令。此外，由于一些网站没有 `robots.txt` 文件，或是URL的格式存在问题，因此我们添加了一些额外的错误处理代码段以及一个新的变量 `no_robots` ，从而可以让我们在无法找到 `robots.txt` 文件时，仍然可以继续爬取。最后，我们添加了 `socket.setdefaulttimeout(60)` ，用于为 `robotparser` 以及第3章中 `Downloader` 类额外的 `timeout` 参数处理超时。

处理本例的主要代码位于本书源码文件的chp4文件夹中，其名为 `advanced_link_crawler.py` 。新的爬虫后续可以直接被 `AlexaCallback` 使用，类似如下所示，在命令行中运行。

```python
python chp4/advanced_link_crawler.py
...
Total time: 1349.7983705997467s
```

查看运行在文件 `__main__` 区域的代码，可以发现我们使用了 `'$^'` 作为模式，避免收集每个页面的链接。你也可以尝试使用 `'.'` 匹配所有内容，爬取每个页面上的所有链接。（警告：这将花费很长时间，很可能以天计！）

在串行下载时，只爬取第一个页面所花费的时间和预期一致，大约为每个URL平均2.7秒（包含测试 `robots.txt` 文件的时间）。因为你的网络运营商速度不同，以及你可能是在云服务器上运行脚本，因此你可能会得到速度更快的结果。

