[toc]

### 1. 学习 wget

`wget` 能够将 Web 页面下载到本地 Linux 系统中：

```shell
$ wget www.quotationspage.com/qotd.html
--2022-09-23 14:12:54--  http://www.quotationspage.com/qotd.html
正在解析主机 www.quotationspage.com (www.quotationspage.com)... 74.208.47.119
正在连接 www.quotationspage.com (www.quotationspage.com)|74.208.47.119|:80... 已连接。
已发出 HTTP 请求，正在等待回应... 200 OK
长度： 未指定 [text/html]
正在保存至: “qotd.html”

qotd.html               [ <=>                ]  12.86K  --.-KB/s    in 0.001s  

2022-09-23 14:12:59 (20.6 MB/s) - “qotd.html” 已保存 [13172]

```

可以使用 `-o` 选项将会话输出保存在日志文件中，随后再浏览：

```shell
$ wget -o quote.log www.quotationspage.com/qotd.html 
$ 
$ cat quote.log 
--2022-09-23 15:23:14--  http://www.quotationspage.com/qotd.html
正在解析主机 www.quotationspage.com (www.quotationspage.com)... 74.208.47.119
正在连接 www.quotationspage.com (www.quotationspage.com)|74.208.47.119|:80... 已连接。
已发出 HTTP 请求，正在等待回应... 200 OK
长度： 未指定 [text/html]
正在保存至: “qotd.html.1”

     0K .......... ..                                          2.47M=0.005s

2022-09-23 15:23:17 (2.47 MB/s) - “qotd.html.1” 已保存 [13172]

```

> 提示：出于各种原因，你可能不希望 `wget` 生成日志文件或显示会话输出。如果是这样的话，可以使用 `-q` 选项，`wget` 命令会安安静静地完成你下达给它的任务。

要控制 Web 页面信息保存的位置，可以使用 `wget` 命令的 `-O` 选项：

```shell
$ wget -o quote.log -O Daily_Quote.html www.quotationspage.com/qotd.html
$ 
$ cat Daily_Quote.html 

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">

<html xmlns:fb="http://ogp.me/ns/fb#">
<head>
[...]
```

### 2. 测试 Web 地址

Web 地址有时候似乎每天都在变，所以在脚本中测试地址的有效性就非常重要。可以使用 `wget` 工具的 `--spider` 选项完成这些任务：

```shell
$ wget --spider www.quotationspage.com/qotd.html
开启 Spider 模式。检查是否存在远程文件。
--2022-09-23 16:32:00--  http://www.quotationspage.com/qotd.html
正在解析主机 www.quotationspage.com (www.quotationspage.com)... 74.208.47.119
正在连接 www.quotationspage.com (www.quotationspage.com)|74.208.47.119|:80... 已连接。
已发出 HTTP 请求，正在等待回应... 200 OK
长度： 未指定 [text/html]
存在远程文件且该文件可能含有更深层的链接，
但不能进行递归操作 -- 无法获取。
```

上面的命令输出的内容太多，可以加上 `-nv` 选项来精简输出信息：

```shell
$ wget -nv --spider www.quotationspage.com/qotd.html2022-09-23 16:33:29 URL: http://www.quotationspage.com/qotd.html 200 OK
```

不过和你认为的恰恰相反，行尾的 OK 并不是说 Web 地址是有效的，而是表明返回的 Web 地址和发送的地址是一样的。下面看一个错误的 Web 地址：

```shell
$ wget -nv --spider www.quotationspage.com/BAD_URL.html
2022-09-23 16:35:35 URL: http://www.quotationspage.com/error404.html 200 OK
```

> 注意：输出的最后仍然是 OK。但是 Web 地址的结尾是 error404.html。这才表示 Web 地址是无效的。

### 3. 创建脚本

```shell
#!/bin/bash
# 
# Get a Daily Inspirational Quote
############################################
#
# Script Variables ####
#
quote_url=www.quotationspage.com/qotd.html
#
# Check url validity ###
#
check_url=$(wget -nv --spider $quote_url 2>&1)
#
if [[ $check_url == *error404* ]]
then
    echo "Bad web address"
    echo "$quote_url invalid"
    echo "Exiting script..."
    exit 
fi
#
# Download Web Site's Information
#
wget -o /tmp/quote.log -O /tmp/quote.html $quote_url
#
# Extract the Desired Data
#
sed 's/<[^>]*//g' /tmp/quote.html |
grep "$(date  -d 'yesterday' +%B' '%-d,' '%Y)" -A2 |
sed 's/>//g' |
sed '/&nbsp;/{n ; d}' |
gawk 'BEGIN {FS="$nbsp;"} {print $1}' |
tee /tmp/daily_quote.txt > /dev/null
#
exit
```

